# ğŸ¯ TÃ©cnicas AgnÃ³sticas de Preprocesamiento - Ingestion Service

## Compatibilidad

| Componente | VersiÃ³n MÃ­nima | VersiÃ³n Recomendada |
|------------|---------------|---------------------|
| **Qdrant** | 1.10+ | **1.16+** |
| **qdrant-client** | 1.9+ | 1.12+ |
| **Python** | 3.10+ | 3.11+ |

### Features por VersiÃ³n de Qdrant

| Feature | Qdrant 1.10 | Qdrant 1.14 | Qdrant 1.16 |
|---------|-------------|-------------|-------------|
| Query API + Prefetch | âœ… | âœ… | âœ… |
| RRF Fusion | âœ… | âœ… | âœ… |
| DBSF Fusion | âœ… (1.11+) | âœ… | âœ… |
| FormulaQuery (Score-Boosting) | âŒ | âœ… | âœ… |
| RRF parametrizado (k) | âŒ | âŒ | âœ… |
| Full-Text Index | âœ… | âœ… | âœ… mejorado |

---

## Resumen

Este mÃ³dulo implementa **4 tÃ©cnicas agnÃ³sticas avanzadas** de preprocesamiento para mejorar significativamente la calidad del RAG:

| TÃ©cnica | Campo Generado | Uso en Qdrant | Beneficio |
|---------|---------------|---------------|-----------|
| **Contextual Injected Chunking** | `content_contextualized` | Vector Denso | Mejor calidad de embedding |
| **Search Anchors** | `search_anchors` | BM25 + Full-Text | Mejor recall (encuentra lo que busca el usuario) |
| **Fact Density** | `fact_density` | Score-Boosting | Prioriza contenido valioso |
| **Entity Normalization** | `normalized_entities` | Filtrado | BÃºsqueda estructurada |

---

## ğŸ”¥ Las 4 TÃ©cnicas Explicadas

### 1. Contextual Injected Chunking

**Problema:** Un chunk de la pÃ¡gina 50 pierde sentido sin contexto.

**SoluciÃ³n:** El LLM genera un `contextual_prefix` de 1-2 frases que sitÃºa el chunk en contexto.

```
ANTES (pierdes contexto):
"El pago serÃ¡ de 500â‚¬"

DESPUÃ‰S (contexto inyectado):
"En el contexto del contrato de servicios de limpieza entre Empresa A 
y Empresa B para el perÃ­odo Marzo 2024: El pago serÃ¡ de 500â‚¬"
```

**Impacto:**
- El embedding captura mejor el significado real
- El LLM final entiende el chunk sin necesitar mÃ¡s contexto

---

### 2. Search Anchors (Queries SintÃ©ticas)

**Problema:** El usuario busca "dolor de cabeza" pero el documento dice "cefalea tensional".

**SoluciÃ³n:** El LLM genera 5-10 formas en que un humano buscarÃ­a esa informaciÃ³n.

```json
{
  "chunk": "La cefalea tensional se caracteriza por...",
  "search_anchors": [
    "dolor de cabeza",
    "cefalea tensional sÃ­ntomas", 
    "quÃ© es la cefalea",
    "dolor cabeza estrÃ©s",
    "tensiÃ³n muscular cabeza"
  ]
}
```

**Uso en Qdrant:**
- Se concatenan y se indexan en Full-Text Index
- Se incluyen en el texto para BM25
- La bÃºsqueda encuentra el chunk aunque el usuario use tÃ©rminos diferentes

---

### 3. Fact Density (Hechos AtÃ³micos)

**Problema:** Â¿CÃ³mo saber quÃ© chunks son mÃ¡s valiosos objetivamente?

**SoluciÃ³n:** 
1. Extraer hechos concretos (`atomic_facts`)
2. Calcular densidad de hechos (`fact_density` 0-1)

```json
{
  "chunk": "La empresa, fundada en 1990 por Juan PÃ©rez en Madrid, 
            se dedica al sector del acero inoxidable.",
  "atomic_facts": [
    "AÃ±o fundaciÃ³n: 1990",
    "Fundador: Juan PÃ©rez",
    "UbicaciÃ³n: Madrid",
    "Sector: Acero inoxidable"
  ],
  "fact_density": 0.85
}
```

**Uso en Qdrant:**
- `fact_density` se usa en Score-Boosting durante bÃºsqueda
- Chunks con mÃ¡s datos concretos suben en el ranking
- `atomic_facts` se indexan para bÃºsqueda exacta

---

### 4. Entity Normalization

**Problema:** El mismo concepto tiene mil nombres (CIF, VAT ID, NIF, Tax ID).

**SoluciÃ³n:** El LLM normaliza entidades a un estÃ¡ndar.

```json
{
  "original": "El CIF de la empresa es B12345678",
  "normalized_entities": {
    "organization_id": "B12345678",
    "entity_type": "company"
  }
}
```

---

## ğŸ“ Estructura de Archivos

```
ingestion_service/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ingestion_models.py      # ChunkModel con campos agnÃ³sticos
â”‚   â””â”€â”€ preprocessing_models.py   # EnrichedChunk, DocumentContext, etc.
â”‚
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ document_preprocess.py    # Prompts agnÃ³sticos optimizados
â”‚
â”œâ”€â”€ handler/
â”‚   â”œâ”€â”€ preprocess_handler.py     # AgnosticPreprocessHandler
â”‚   â”œâ”€â”€ document_handler.py       # IntegraciÃ³n con preprocesamiento
â”‚   â””â”€â”€ qdrant_handler.py         # Almacenamiento + bÃºsqueda agnÃ³stica
â”‚
â””â”€â”€ config/
    â””â”€â”€ settings.py               # ConfiguraciÃ³n de preprocessing
```

---

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

```bash
# Habilitar preprocesamiento agnÃ³stico
ENABLE_DOCUMENT_PREPROCESSING=true

# API key de Groq
GROQ_API_KEY=gsk_your_api_key_here

# Modelo LLM para preprocesamiento
PREPROCESSING_MODEL=deepseek-r1-distill-llama-70b

# Tokens mÃ¡ximos por bloque
PREPROCESSING_MAX_TOKENS_PER_BLOCK=3000
```

### ConfiguraciÃ³n en RAGIngestionConfig

```python
rag_config = RAGIngestionConfig(
    embedding_model=EmbeddingModel.TEXT_EMBEDDING_3_SMALL,
    chunk_size=512,
    chunk_overlap=50,
    enable_preprocessing=True,    # Habilitar preprocesamiento
    fact_density_boost=0.3        # Peso del boost (0-1)
)
```

---

## ğŸ“Š Flujo de Procesamiento

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Documento       â”‚
â”‚ (PDF/DOCX/etc)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ExtracciÃ³n      â”‚  PyMuPDF/python-docx
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunking        â”‚  SentenceSplitter (raw chunks)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Contexto Doc    â”‚  LLM genera summary + topics (UNA VEZ)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Enriquecimiento â”‚  LLM genera por cada chunk:
â”‚ AgnÃ³stico       â”‚  - contextual_prefix
â”‚                 â”‚  - search_anchors
â”‚                 â”‚  - atomic_facts
â”‚                 â”‚  - fact_density
â”‚                 â”‚  - normalized_entities
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding       â”‚  Del content_contextualized (con prefijo)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qdrant          â”‚  Vectores + payload agnÃ³stico + Ã­ndices
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Estructura del Chunk Almacenado

```json
{
  "chunk_id": "uuid",
  "document_id": "uuid",
  "tenant_id": "uuid",
  "collection_id": "col_abc123",
  
  "content": "En el contexto del contrato de servicios...\n\nEl pago serÃ¡ de 500â‚¬",
  "content_raw": "El pago serÃ¡ de 500â‚¬",
  
  "search_anchors": "pago contrato cuÃ¡nto cuesta precio servicios limpieza monto total",
  "atomic_facts": "Monto: 500 Moneda: EUR Tipo: pago servicios",
  "fact_density": 0.75,
  "document_nature": "transactional",
  "normalized_entities": {
    "amount": "500",
    "currency": "EUR"
  },
  
  "embedding_model": "text-embedding-3-small",
  "embedding_dimensions": 1536,
  
  "metadata": {
    "document_name": "Contrato_Limpieza.pdf",
    "preprocessing_used": true,
    "contextual_prefix": "En el contexto del contrato de servicios..."
  }
}
```

---

## ğŸ” BÃºsqueda con Score-Boosting (Qdrant 1.14+)

La implementaciÃ³n usa **FormulaQuery nativo** de Qdrant para hacer el boost directamente en el servidor:

```python
# En query_service (ejemplo)
results = await qdrant_handler.search_hybrid_with_boost(
    tenant_id=tenant_id,
    agent_id=agent_id,
    query_dense=embedding,
    query_sparse=sparse_vector,
    fact_density_boost=0.3,  # Priorizar chunks con mÃ¡s datos
    rrf_k=60,                # ParÃ¡metro RRF (Qdrant 1.16+)
    limit=10
)
```

### FormulaQuery Interno

La bÃºsqueda usa FormulaQuery nativo de Qdrant 1.14+:

```python
# FÃ³rmula ejecutada en Qdrant (no en Python)
FormulaQuery(
    formula=SumExpression(sum=[
        "$score",  # Score del RRF/DBSF
        MultExpression(mult=[
            0.3,           # fact_density_boost
            "fact_density" # Payload key
        ])
    ]),
    defaults={"fact_density": 0.5}
)
```

### MÃ©todos de BÃºsqueda Disponibles

| MÃ©todo | DescripciÃ³n | Qdrant MÃ­nimo |
|--------|-------------|---------------|
| `search_hybrid_with_boost()` | RRF + FormulaQuery boost | 1.14+ |
| `search_hybrid_dbsf()` | DBSF + FormulaQuery boost | 1.14+ |
| `search_in_anchors()` | Full-Text en search_anchors | 1.10+ |
| `search_in_facts()` | Full-Text en atomic_facts | 1.10+ |
| `search_by_agent()` | BÃºsqueda simple (legacy) | 1.10+ |

### Fallback AutomÃ¡tico

Si FormulaQuery falla (Qdrant < 1.14), el sistema hace fallback a boost manual en Python:

```python
# _fallback_hybrid_search() se activa automÃ¡ticamente
boosted_score = score + (fact_density_boost * fact_density)
```

---

## ğŸ“ˆ MÃ©tricas de Ã‰xito

### En PreprocessingResult

```python
result = await preprocess_handler.preprocess_document(...)

print(f"Chunks procesados: {result.total_chunks}")
print(f"Densidad promedio: {result.avg_fact_density}")
print(f"Total search anchors: {result.total_search_anchors}")
print(f"Total atomic facts: {result.total_atomic_facts}")
print(f"Tokens LLM usados: {result.llm_usage['total_tokens']}")
```

### Indicadores de Calidad

| MÃ©trica | Valor Bajo | Valor Alto |
|---------|------------|------------|
| `avg_fact_density` | < 0.3 (texto vago) | > 0.7 (muchos datos) |
| `total_search_anchors` | < 3 por chunk | > 7 por chunk |
| `total_atomic_facts` | 0 (sin datos concretos) | > 5 por chunk |

---

## âš ï¸ Fallback AutomÃ¡tico

Si el preprocesamiento falla:
1. Se registra error en logs
2. Se usa chunking tradicional (SentenceSplitter)
3. Los chunks se crean con valores por defecto:
   - `search_anchors`: vacÃ­o
   - `fact_density`: 0.5
   - `document_nature`: "other"
4. `metadata.preprocessing_used = false`

---

## ğŸ’° Costos Estimados (Groq)

| Documento | Chunks | Tokens Input | Tokens Output | Costo |
|-----------|--------|--------------|---------------|-------|
| 5,000 palabras | ~10 | ~15,000 | ~5,000 | ~$0.08 |
| 10,000 palabras | ~20 | ~30,000 | ~10,000 | ~$0.16 |
| 20,000 palabras | ~40 | ~60,000 | ~20,000 | ~$0.32 |

*Basado en precios de Groq para modelos 70B (Diciembre 2024)*

---

## ğŸš€ PrÃ³ximos Pasos para Query Service

Para completar la implementaciÃ³n, el **query_service** necesita:

1. **Usar `search_hybrid_with_boost`** en lugar de bÃºsqueda simple
2. **Pasar `fact_density_boost`** desde configuraciÃ³n del agente
3. **Opcionalmente** buscar en `search_anchors` y `atomic_facts` para casos especÃ­ficos

Ejemplo de integraciÃ³n:

```python
# En query_service/handler/qdrant_handler.py

async def search_for_context(
    self,
    query: str,
    tenant_id: str,
    agent_id: str,
    fact_density_boost: float = 0.3
):
    # Generar embeddings
    dense = await self.embedding_client.embed(query)
    sparse = self.sparse_model.embed([query])[0]
    
    # BÃºsqueda hÃ­brida con boost
    results = await self.client.search_hybrid_with_boost(
        tenant_id=tenant_id,
        agent_id=agent_id,
        query_dense=dense,
        query_sparse=sparse,
        fact_density_boost=fact_density_boost,
        limit=10
    )
    
    # Los resultados ya vienen ordenados por boosted_score
    return results
```
